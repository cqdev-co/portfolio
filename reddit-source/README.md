# Reddit Source - Enterprise Financial Data Pipeline

An enterprise-grade Reddit data source for financial sentiment analysis and trading signal enhancement. This system ingests Reddit posts from financial subreddits, processes them through OCR and VLM pipelines, extracts structured financial data, and provides clean datasets for integration with trading scanners.

## ğŸš€ Features

### Core Capabilities
- **Multi-subreddit ingestion** with intelligent rate limiting and error handling
- **Enterprise-grade OCR pipeline** using PaddleOCR for chart and screenshot analysis
- **Local VLM processing** with Qwen2-VL for structured data extraction
- **Ticker verification system** with collision detection and market sanity checks
- **Quality scoring and filtering** to separate valuable content from noise
- **Market data enrichment** with price snapshots and forward returns
- **Real-time feature aggregation** with buzz scores and sentiment analysis
- **Scanner integration** for volatility squeeze and other trading strategies

### Technical Excellence
- **Supabase integration** for scalable cloud storage
- **Async/await architecture** for high-performance concurrent processing
- **Comprehensive error handling** with retry logic and circuit breakers
- **Rich CLI interface** with progress tracking and colored output
- **Streamlit audit UI** for data inspection and quality monitoring
- **Full test coverage** with unit and integration tests
- **Production-ready logging** with structured JSON output

## ğŸ“‹ Requirements

- Python 3.11+
- Poetry for dependency management
- Supabase account for data storage
- Reddit API credentials
- 8GB+ RAM (for local VLM processing)
- Apple Silicon Mac (for MPS acceleration) or CUDA GPU

## ğŸ›  Installation

### 1. Clone and Setup
```bash
git clone <repository-url>
cd reddit-source
make setup
```

### 2. Configure Environment
```bash
cp .env.example .env
# Edit .env with your credentials
```

Required environment variables:
```bash
# Reddit API
REDDIT_CLIENT_ID=your_reddit_client_id
REDDIT_CLIENT_SECRET=your_reddit_client_secret
REDDIT_USER_AGENT=reddit-source/0.1 by yourusername

# Supabase
SUPABASE_URL=your_supabase_project_url
SUPABASE_KEY=your_supabase_anon_key
SUPABASE_SERVICE_KEY=your_supabase_service_role_key
```

### 3. Database Setup
Execute the SQL schema in your Supabase dashboard:
```bash
# Copy contents of db/reddit_source_schema.sql
# Run in Supabase SQL Editor
```

### 4. Initialize System
```bash
reddit-source init
reddit-source status  # Verify connection
```

## ğŸš€ Quick Start

### Basic Usage
```bash
# Single ingestion run
reddit-source ingest --once

# Continuous monitoring
reddit-source ingest --watch

# Export data for analysis
reddit-source export --date 2025-09-22

# Join with scanner signals
reddit-source join --scanner path/to/scanner_signals.parquet

# Launch audit interface
reddit-source audit
```

### Advanced Usage
```bash
# Custom subreddit list
reddit-source ingest --subreddits "stocks,options,wallstreetbets" --limit 200

# Export specific ticker
reddit-source export --date 2025-09-22 --ticker AAPL --format csv

# Debug mode
reddit-source ingest --once --debug
```

## ğŸ“Š Data Pipeline

### 1. Ingestion
- Polls configured subreddits every 10 minutes
- Downloads images and stores metadata
- Handles rate limiting and authentication
- Deduplicates posts automatically

### 2. OCR & Routing
- Processes images with PaddleOCR
- Routes content: chart | pnl | slide | meme | other
- Extracts text tokens with confidence scores
- Optional CLIP-based classification

### 3. VLM Extraction
- Local Qwen2-VL processing for structured data
- Tool-calling JSON output with validation
- Extracts tickers, sentiment, price levels
- Confidence scoring for each field

### 4. Ticker Verification
- Symbol table with collision detection
- Context window analysis
- Market sanity checks (price validation)
- False positive reduction >70%

### 5. Quality Scoring
- Multi-factor scoring algorithm
- Content, evidence, and engagement metrics
- Automatic quarantine of low-quality posts
- Spam and duplicate detection

### 6. Market Enrichment
- Real-time price snapshots via yfinance
- Forward return calculations (1d, 5d, 30d)
- Earnings calendar integration
- IV rank and volume data

### 7. Feature Aggregation
- 2-hour time buckets for buzz calculation
- Z-scored metrics for anomaly detection
- Sentiment aggregation and author diversity
- Trade plan and chart detection flags

## ğŸ“ˆ Scanner Integration

### Volatility Squeeze Integration
```python
import pandas as pd
from reddit_source import get_storage_client

# Get Reddit signals
storage = get_storage_client()
reddit_signals = await storage.get_scanner_ready_signals(
    min_buzz_z=2.0,
    quality_tiers=["valuable"]
)

# Load your scanner signals
scanner_df = pd.read_parquet("volatility_squeeze_signals.parquet")

# Join on ticker and time
joined_df = scanner_df.merge(
    reddit_signals,
    left_on=["ticker", "signal_ts"],
    right_on=["primary_ticker", "created_datetime"],
    how="left"
)

# Filter for Reddit-confirmed signals
confirmed_signals = joined_df[
    (joined_df["buzz_z"] >= 2.0) & 
    (joined_df["bull_pct"] >= 0.6)
]
```

### Feature Engineering
Available Reddit features for each ticker/time bucket:
- `post_count`: Number of mentions
- `unique_authors`: Author diversity
- `bull_pct`: Bullish sentiment percentage
- `buzz_z`: Z-scored buzz metric
- `has_tradeplan`: Contains entry/stop/target levels
- `has_chart`: Contains technical analysis
- `avg_sentiment`: Average sentiment score
- `valuable_count`: High-quality posts count

## ğŸ”§ Configuration

### Subreddit Configuration
```python
# Default financial subreddits
SUBREDDITS = [
    "stocks", "investing", "options", "Daytrading", 
    "wallstreetbets", "pennystocks", "biotech", 
    "semiconductors", "ETFs", "SecurityAnalysis",
    "ValueInvesting", "financialindependence"
]
```

### Quality Thresholds
```python
# Value scoring thresholds
VALUE_SCORE_KEEP_THRESHOLD = 0.60      # Keep as valuable
VALUE_SCORE_QUARANTINE_THRESHOLD = 0.30 # Soft quarantine
# Below 0.30 = hard drop

# Engagement filters
MIN_SCORE_THRESHOLD = 5
MIN_COMMENT_THRESHOLD = 2
```

### VLM Configuration
```python
# Local VLM settings
VLM_MODEL_NAME = "Qwen/Qwen2-VL-7B-Instruct"
VLM_DEVICE = "mps"  # or "cuda"
VLM_MAX_TOKENS = 2048
VLM_TEMPERATURE = 0.1
```

## ğŸ“Š Monitoring & Audit

### Streamlit Dashboard
```bash
reddit-source audit
# Opens http://localhost:8501
```

Features:
- Real-time processing statistics
- Post quality distribution
- Image classification results
- Ticker extraction accuracy
- Sentiment analysis trends
- Error rate monitoring

### CLI Status
```bash
reddit-source status
```

Shows:
- Supabase connection health
- Processing pipeline status
- Daily ingestion counts
- Quality tier distribution
- Recent error summary

## ğŸ§ª Testing

### Run Test Suite
```bash
# All tests
make test

# Unit tests only
make test-unit

# Integration tests
make test-integration

# With coverage
make test-coverage
```

### Test Categories
- **Unit tests**: Individual component testing
- **Integration tests**: End-to-end pipeline testing
- **Performance tests**: Load and stress testing
- **Data quality tests**: Output validation

## ğŸ“ˆ Performance

### Benchmarks (M3 Pro MacBook)
- **Ingestion**: 1000 posts/minute
- **OCR processing**: 50 images/minute
- **VLM extraction**: 20 images/minute
- **Database writes**: 500 records/second
- **Memory usage**: ~4GB peak (with VLM)

### Optimization Features
- Async/await for I/O concurrency
- Batch database operations
- Image compression and resizing
- Intelligent caching strategies
- Rate limit compliance

## ğŸ”’ Security & Privacy

### Data Handling
- No personal information stored
- Images processed locally only
- Configurable data retention
- GDPR compliance ready

### API Security
- OAuth2 authentication
- Rate limit compliance
- Request timeout handling
- Error sanitization

## ğŸ¤ Integration Examples

### With Volatility Squeeze Scanner
```python
# In your scanner code
from reddit_source import get_storage_client

async def enhance_signals_with_reddit(signals_df):
    storage = get_storage_client()
    
    for _, signal in signals_df.iterrows():
        reddit_data = await storage.get_scanner_ready_signals(
            ticker=signal['ticker'],
            min_buzz_z=1.5
        )
        
        if reddit_data:
            signal['reddit_buzz'] = reddit_data[0]['buzz_z']
            signal['reddit_sentiment'] = reddit_data[0]['bull_pct']
    
    return signals_df
```

### Custom Feature Pipeline
```python
from reddit_source.storage import get_admin_storage_client
from reddit_source.ingest.models import RedditFeatures

async def compute_custom_features(ticker: str, start_time: datetime):
    storage = get_admin_storage_client()
    
    # Your custom aggregation logic
    features = RedditFeatures(
        ticker=ticker,
        bucket_2h=start_time,
        custom_metric=your_calculation()
    )
    
    await storage.insert_reddit_features(features)
```

## ğŸ“š API Reference

### Storage Client
```python
from reddit_source.storage import get_storage_client, get_admin_storage_client

# Read operations
storage = get_storage_client()
posts = await storage.get_posts_by_subreddit("stocks", limit=100)
signals = await storage.get_scanner_ready_signals(ticker="AAPL")

# Write operations (requires admin client)
admin_storage = get_admin_storage_client()
await admin_storage.insert_post(post_data)
await admin_storage.update_reddit_features_for_ticker("AAPL", start, end)
```

### Reddit Client
```python
from reddit_source.ingest import RedditClient

async with RedditClient() as reddit:
    posts = await reddit.get_recent_posts(window_minutes=120)
    image_downloaded = await reddit.download_image(url, path)
```

## ğŸ›£ Roadmap

### Phase 1 (Current)
- âœ… Core ingestion pipeline
- âœ… OCR and VLM processing
- âœ… Supabase integration
- âœ… CLI interface
- âœ… Basic quality filtering

### Phase 2 (Next)
- ğŸ”„ Advanced sentiment analysis
- ğŸ”„ Real-time streaming pipeline
- ğŸ”„ Multi-language support
- ğŸ”„ Enhanced duplicate detection
- ğŸ”„ Performance optimizations

### Phase 3 (Future)
- ğŸ“‹ Machine learning models
- ğŸ“‹ Alternative data sources
- ğŸ“‹ Real-time alerts
- ğŸ“‹ Advanced visualization
- ğŸ“‹ API service layer

## ğŸ› Troubleshooting

### Common Issues

**Authentication Errors**
```bash
# Check Reddit credentials
reddit-source status

# Verify .env configuration
cat .env | grep REDDIT
```

**Supabase Connection Issues**
```bash
# Test connection
reddit-source status

# Check service key permissions
# Ensure RLS policies are configured
```

**VLM Processing Errors**
```bash
# Check device availability
python -c "import torch; print(torch.backends.mps.is_available())"

# Reduce batch size if OOM
export VLM_BATCH_SIZE=1
```

**Image Download Failures**
```bash
# Check network connectivity
# Verify image storage permissions
ls -la data/images/
```

### Performance Tuning

**High Memory Usage**
- Reduce VLM batch size
- Enable image compression
- Implement processing queues

**Slow Processing**
- Increase concurrent requests
- Use faster storage (SSD)
- Optimize database indexes

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“ Support

- GitHub Issues: Bug reports and feature requests
- Documentation: Comprehensive guides and examples
- Community: Discord server for discussions

---

**Built with â¤ï¸ for the trading community**
