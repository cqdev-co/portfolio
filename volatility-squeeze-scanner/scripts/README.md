# Volatility Squeeze Scanner - Utility Scripts

This directory contains utility scripts for maintaining and managing the volatility squeeze scanner database.

## Scripts Overview

| Script | Purpose | Key Features |
|--------|---------|--------------|
| `export_signals_data.py` | Export signals and performance data | Date filtering, JSON export, multiple data sources |
| `clean_signals.py` | Clean signals by score threshold | Score-based filtering, comprehensive analysis, safe deletion |
| `clean_table.py` | Clean duplicate entries | Duplicate detection, ticker deduplication, date-based cleanup |
| `cleanup_meaningless_trades.py` | Clean 0.0% return trades | Performance dashboard cleanup, GitHub Actions integration |
| `fix_duplicate_signals.py` | Fix signal duplicates | Signal continuity repair, cross-date duplicate resolution |

## export_signals_data.py

A comprehensive data export utility for extracting unusual options signals, signal performance data, and volatility squeeze signals from the database. This script provides flexible date filtering and exports data to well-formatted JSON files for analysis, reporting, or backup purposes.

### Features

- **Multiple Data Sources**: Export unusual options signals, signal performance data, and volatility squeeze signals
- **Flexible Date Filtering**: Filter by days back, specific date ranges, or export all data
- **JSON Export**: Clean, formatted JSON output with proper date serialization
- **Progress Tracking**: Real-time progress indicators and detailed summaries
- **Error Handling**: Robust error handling with detailed logging
- **CLI Interface**: Easy-to-use command-line interface with comprehensive options

### Usage

```bash
# Activate the virtual environment first
source ../venv/bin/activate

# Show help
python export_signals_data.py --help

# BASIC USAGE
# Export last 7 days of data (default if no filter specified)
python export_signals_data.py --days 7

# Export last 30 days of data
python export_signals_data.py --days 30

# Export specific date range
python export_signals_data.py --start-date 2024-01-01 --end-date 2024-01-31

# Export all available data (use with caution for large datasets)
python export_signals_data.py --all

# SELECTIVE EXPORTS
# Export only unusual options signals
python export_signals_data.py --days 7 --unusual-options-only

# Export only signal performance data
python export_signals_data.py --days 7 --performance-only

# Export only volatility squeeze signals
python export_signals_data.py --days 7 --volatility-squeeze-only

# CUSTOM OUTPUT
# Specify custom output directory
python export_signals_data.py --days 7 --output ./my_exports

# Enable verbose logging
python export_signals_data.py --days 7 --verbose
```

### Output Files

The script generates timestamped JSON files in the specified output directory:

- `unusual_options_signals_YYYYMMDD_HHMMSS.json` - Unusual options activity data
- `signal_performance_YYYYMMDD_HHMMSS.json` - Signal performance tracking data
- `volatility_squeeze_signals_YYYYMMDD_HHMMSS.json` - Volatility squeeze signals data

### Data Sources

**Unusual Options Signals** (`unusual_options_signals` table):
- Option contract details (strike, expiry, type)
- Volume and open interest metrics
- Premium flow and aggressive order data
- Detection flags and scoring
- Risk assessment and market context

**Signal Performance** (`signal_performance` table):
- Entry and exit data for tracked signals
- Forward returns (1d, 5d, 30d)
- Win/loss classification
- Performance metrics and risk data

**Volatility Squeeze Signals** (`volatility_squeeze_signals` table):
- Squeeze detection and analysis
- Technical indicators and price data
- AI analysis and recommendations
- Signal continuity tracking

### Output Example

```
üìä DATA EXPORT SUMMARY
============================================================
üìÖ Date Range: Last 7 days (from 2024-10-22)
üîç Unusual Options Signals: 1,234 records
üìà Signal Performance Data: 456 records
üéØ Volatility Squeeze Signals: 789 records
üìä Total Records Exported: 2,479
============================================================
üìÅ Files exported to: /Users/user/exports
```

### Sample JSON Structure

**Unusual Options Signals:**
```json
{
  "signal_id": "uuid",
  "ticker": "AAPL",
  "option_symbol": "AAPL241025C00225000",
  "detection_timestamp": "2024-10-22T14:30:00Z",
  "strike": 225.00,
  "expiry": "2024-10-25",
  "option_type": "call",
  "current_volume": 5000,
  "volume_ratio": 15.2,
  "overall_score": 0.85,
  "grade": "A"
}
```

**Signal Performance:**
```json
{
  "id": "uuid",
  "symbol": "AAPL",
  "entry_date": "2024-10-15",
  "entry_price": 220.50,
  "exit_price": 225.75,
  "return_pct": 2.38,
  "days_held": 3,
  "is_winner": true,
  "status": "CLOSED"
}
```

### Environment Requirements

- Python 3.8+
- Virtual environment with scanner dependencies
- Supabase credentials in environment variables:
  - `NEXT_PUBLIC_SUPABASE_URL` or `SUPABASE_URL`
  - `NEXT_PUBLIC_SUPABASE_SERVICE_ROLE_KEY` or `NEXT_PUBLIC_SUPABASE_ANON_KEY`

### Best Practices

1. **Start Small**: Begin with short date ranges to test connectivity and performance
2. **Use Selective Exports**: Export only the data you need to reduce file sizes
3. **Monitor Progress**: Use `--verbose` for detailed logging during large exports
4. **Organize Outputs**: Use descriptive output directory names for different export purposes
5. **Regular Exports**: Set up regular exports for backup and analysis purposes

### Use Cases

- **Data Analysis**: Export data for external analysis tools (Python, R, Excel)
- **Reporting**: Generate reports for stakeholders or compliance
- **Backup**: Create regular backups of critical signal data
- **Integration**: Feed data into other systems or dashboards
- **Research**: Historical analysis of signal performance and market conditions

### Troubleshooting

**Connection Issues:**
- Verify Supabase credentials are correctly set
- Check network connectivity
- Ensure credentials have read permissions

**Large Datasets:**
- Use date filtering to reduce data size
- Consider selective exports for specific data types
- Monitor available disk space for output files

**No Data Found:**
- Verify the date range contains data
- Check if the specific data tables exist
- Ensure proper database permissions

## clean_signals.py

A comprehensive script for cleaning volatility squeeze signals based on minimum score thresholds. This tool helps maintain database quality by removing low-scoring signals while preserving valuable data.

### Features

- **Score-Based Filtering**: Remove signals below specified threshold (0.0-1.0)
- **Comprehensive Analysis**: Score distribution, recommendation breakdown, opportunity rank analysis
- **Safety Features**: Automatic backup, confirmation prompts, dry-run mode
- **Flexible Filtering**: Date-specific or time-range based cleanup
- **Rich Reporting**: Color-coded output with detailed statistics

### Usage

```bash
# Activate the virtual environment first
source ../venv/bin/activate

# Show help
python clean_signals.py --help

# BASIC USAGE
# Preview cleanup for signals with score < 0.80
python clean_signals.py --min-score 0.80 --dry-run

# Actually clean signals with score < 0.80
python clean_signals.py --min-score 0.80

# TARGETED CLEANUP
# Clean signals for specific date
python clean_signals.py --min-score 0.90 --date 2024-01-15

# Clean signals from last 7 days
python clean_signals.py --min-score 0.75 --days-back 7

# BACKUP OPERATIONS
# Create backup only (no cleanup)
python clean_signals.py --min-score 0.60 --backup-only

# Custom backup path
python clean_signals.py --min-score 0.70 --backup-path /path/to/backup.json
```

### Safety Considerations

1. **Always dry-run first**: Use `--dry-run` to preview changes
2. **Start conservative**: Begin with lower thresholds (e.g., 0.3) and work up
3. **Check actionable signals**: Pay attention to warnings about actionable signals
4. **Verify backups**: Ensure backup files are created and accessible

### Output Example

```
üßπ Volatility Squeeze Scanner - Signal Score Cleaner

‚úÖ Connected to Supabase successfully
‚úÖ Backup created: backup_volatility_signals_20240920_143022.json (5000 records)

üìä Signal Score Analysis Results
Date Range: All dates
Minimum Score Threshold: 0.8
Total Signals in Database: 5000
Signals Below Threshold: 1200
Percentage to Clean: 24.0%

üìà Score Range Distribution:
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Score Range ‚îÉ Count ‚îÉ Action               ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ 0.0-0.2     ‚îÇ   300 ‚îÇ Will remove 300      ‚îÇ
‚îÇ 0.2-0.4     ‚îÇ   400 ‚îÇ Will remove 400      ‚îÇ
‚îÇ 0.4-0.6     ‚îÇ   350 ‚îÇ Will remove 350      ‚îÇ
‚îÇ 0.6-0.8     ‚îÇ   150 ‚îÇ Will remove 150      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚ö†Ô∏è  WARNING: 45 actionable signals will be removed!

Proceed with removing 1200 signals below score 0.8? [y/N]: y

‚úÖ Successfully removed 1200 signals below score 0.8

üéâ Cleanup completed successfully! Removed 1200 signals below score 0.8.
```

## clean_table.py

A comprehensive script for cleaning duplicate entries from the `volatility_squeeze_signals` table.

### Features

- **Safe Operations**: Always creates backups before making changes
- **Dry Run Mode**: Preview what would be cleaned without making changes
- **Flexible Filtering**: Clean by specific date, date range, or all records
- **Rich Output**: Beautiful console output with progress indicators
- **Batch Processing**: Handles large datasets efficiently
- **Error Handling**: Robust error handling with detailed logging

### Usage

```bash
# Activate the virtual environment first
source ../venv/bin/activate

# Show help
python clean_table.py --help

# TICKER DEDUPLICATION (recommended for frontend dashboard)
# Analyze duplicate tickers
python clean_table.py --analyze-tickers

# Preview ticker cleanup (safe)
python clean_table.py --clean-tickers --dry-run

# Clean duplicate tickers (keeps most recent record per ticker)
python clean_table.py --clean-tickers

# DATE-BASED DUPLICATE CLEANING
# Dry run - see what would be cleaned (safe)
python clean_table.py --dry-run

# Clean duplicates for a specific date
python clean_table.py --date 2024-01-15

# Clean duplicates for the last 7 days
python clean_table.py --days-back 7

# Clean all duplicates (creates backup first)
python clean_table.py --all

# BACKUP AND MAINTENANCE
# Create backup only (no cleaning)
python clean_table.py --backup-only

# Test database constraints
python clean_table.py --test-constraints

# Clean with custom backup path
python clean_table.py --all --backup-path /path/to/backup.json
```

### Safety Features

1. **Automatic Backups**: Creates JSON backups before any cleanup operation
2. **Confirmation Prompts**: Asks for confirmation before deleting records
3. **Dry Run Mode**: Always test with `--dry-run` first
4. **Detailed Logging**: Shows exactly what will be or was cleaned
5. **Batch Processing**: Handles large datasets without timeouts

### Output Example

```
üßπ Volatility Squeeze Scanner - Table Cleaner

‚úÖ Connected to Supabase successfully
‚úÖ Backup created: backup_volatility_signals_20240115_143022.json (1500 records)

üìä Duplicate Analysis Results
Date Range: Range: 2024-01-08 to 2024-01-15
Total Signals: 1500
Unique Combinations: 1485
Duplicate Records: 15

üîç Duplicate Groups Found:
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Symbol ‚îÉ Scan Date  ‚îÉ Total Count ‚îÉ Duplicates ‚îÉ Action               ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ AAPL   ‚îÇ 2024-01-15 ‚îÇ           2 ‚îÇ          1 ‚îÇ Will remove 1        ‚îÇ
‚îÇ GOOGL  ‚îÇ 2024-01-14 ‚îÇ           3 ‚îÇ          2 ‚îÇ Will remove 2        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Proceed with removing 15 duplicate records? [y/N]: y

‚úÖ Successfully removed 15 duplicate records

üéâ Cleanup completed successfully! Removed 15 duplicates.
```

### Environment Requirements

The script requires the same environment as the main scanner:

- Python 3.8+
- Virtual environment with scanner dependencies
- Supabase credentials in environment variables:
  - `NEXT_PUBLIC_SUPABASE_URL` or `SUPABASE_URL`
  - `NEXT_PUBLIC_SUPABASE_SERVICE_ROLE_KEY` or `NEXT_PUBLIC_SUPABASE_ANON_KEY`

### How It Works

1. **Analysis Phase**: Scans the database for duplicate records (same symbol + scan_date)
2. **Backup Phase**: Creates a JSON backup of all records (if not dry-run)
3. **Cleanup Phase**: Removes older duplicate records, keeping the most recent
4. **Reporting Phase**: Shows detailed results of the cleanup operation

### Best Practices

1. **Always test first**: Use `--dry-run` to see what would be cleaned
2. **Regular maintenance**: Run weekly with `--days-back 7`
3. **Keep backups**: Don't delete backup files immediately
4. **Monitor logs**: Check for any errors during cleanup
5. **Verify results**: Run analysis again after cleanup to confirm

### Troubleshooting

**Connection Issues**:
- Ensure environment variables are set correctly
- Check network connectivity to Supabase
- Verify credentials have proper permissions

**No Duplicates Found**:
- This is normal if the upsert logic is working correctly
- The scanner should prevent duplicates automatically

**Backup Failures**:
- Check disk space for backup files
- Ensure write permissions in the script directory
- Large datasets may take time to backup

### Integration with Scanner

This script complements the scanner's built-in duplicate prevention:

- **Scanner**: Prevents duplicates during data insertion (upsert logic)
- **Cleanup Script**: Removes any duplicates that may have occurred due to race conditions or system issues

The combination ensures data integrity and optimal database performance.

## cleanup_meaningless_trades.py

A specialized script for cleaning meaningless trades (0.0% return) from the performance tracking system. This script is designed to keep the performance dashboard clean and meaningful by removing trades that provide no actionable insights.

### Features

- **Zero-Return Filtering**: Removes trades with exactly 0.0% return regardless of hold time
- **Dashboard Cleanup**: Keeps performance metrics clean and meaningful
- **GitHub Actions Integration**: Designed to run automatically in CI/CD pipeline
- **Dry Run Support**: Preview what would be cleaned without making changes
- **Safe Operations**: Detailed logging and error handling
- **Comprehensive Reporting**: Shows before/after statistics

### Usage

```bash
# Activate the virtual environment first
cd volatility-squeeze-scanner
source ../venv/bin/activate

# Show help
python scripts/cleanup_meaningless_trades.py --help

# BASIC USAGE
# Preview cleanup (safe)
python scripts/cleanup_meaningless_trades.py --dry-run

# Actually clean meaningless trades
python scripts/cleanup_meaningless_trades.py

# POETRY USAGE (recommended)
# Preview cleanup
poetry run python scripts/cleanup_meaningless_trades.py --dry-run

# Run cleanup
poetry run python scripts/cleanup_meaningless_trades.py
```

### GitHub Actions Integration

This script is automatically integrated into the volatility squeeze scanner workflow (`vss.yml`) and runs after each market scan to keep the performance dashboard clean:

```yaml
- name: Cleanup Meaningless Trades
  env:
    NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
    NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
    NEXT_PUBLIC_SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_SERVICE_ROLE_KEY }}
  working-directory: volatility-squeeze-scanner
  run: |
    echo "üßπ Running performance tracking cleanup..."
    poetry run python scripts/cleanup_meaningless_trades.py
    echo "‚úÖ Cleanup complete"
```

### What Gets Cleaned

The script removes performance tracking records where:
- **Return percentage is exactly 0.0%** (within 0.01% tolerance)
- **Any hold time** (0 days, 1 day, or longer)
- **Status is CLOSED** (doesn't affect active trades)

### Output Example

```
üßπ PERFORMANCE TRACKING CLEANUP
==================================================
Mode: LIVE CLEANUP

üìä ANALYSIS RESULTS:
  Total closed trades: 25
  Meaningful trades: 20
  Meaningless trades (0.0% return): 5

‚ùå MEANINGLESS TRADES TO REMOVE:
  ‚Ä¢ PC: $8.32 ‚Üí $8.32 (+0.00%) in 0d [2025-09-24]
  ‚Ä¢ EGG: $6.26 ‚Üí $6.26 (+0.00%) in 0d [2025-09-24]
  ‚Ä¢ ADCT: $3.52 ‚Üí $3.52 (+0.00%) in 1d [2025-09-24]
  ‚Ä¢ SRI: $8.06 ‚Üí $8.06 (+0.00%) in 0d [2025-09-24]
  ‚Ä¢ MSA: $169.97 ‚Üí $169.97 (+0.00%) in 0d [2025-09-24]

üóëÔ∏è  REMOVING MEANINGLESS TRADES...
    ‚úÖ Deleted PC
    ‚úÖ Deleted EGG
    ‚úÖ Deleted ADCT
    ‚úÖ Deleted SRI
    ‚úÖ Deleted MSA

‚úÖ CLEANUP COMPLETE:
  Successfully deleted: 5/5 trades
  Remaining meaningful trades: 20

üìà SAMPLE MEANINGFUL TRADES:
  ‚Ä¢ BBGI: +2.48% in 0d
  ‚Ä¢ ABVC: -1.24% in 0d
  ‚Ä¢ NMG: +2.44% in 0d
  ‚Ä¢ TE: +0.22% in 0d
  ‚Ä¢ CLCO: +0.54% in 0d

‚úÖ CLEANUP COMPLETE: 5 meaningless trades removed
```

### Why This Matters

**Before Cleanup:**
- Performance dashboard cluttered with 0.0% trades
- Misleading win rates and average returns
- Difficult to identify meaningful patterns
- Poor user experience

**After Cleanup:**
- Clean, actionable performance data
- Accurate win rates and return metrics
- Clear identification of profitable strategies
- Professional presentation of results

### Integration with Performance Tracking

This script works in conjunction with the enhanced performance tracking service:

1. **Prevention**: Updated `PerformanceTrackingService` prevents most 0.0% trades from being created
2. **Cleanup**: This script removes any that slip through or were created before the prevention logic
3. **Automation**: GitHub Actions ensures cleanup happens after every scan
4. **Maintenance**: Can be run manually for historical cleanup

### Best Practices

1. **Run after major scans**: Especially useful after full market scans
2. **Monitor output**: Check logs to understand what's being cleaned
3. **Test first**: Use `--dry-run` when running manually
4. **Regular automation**: Let GitHub Actions handle routine cleanup
5. **Historical cleanup**: Run manually on historical data as needed

### Environment Requirements

Same as other scripts:
- Python 3.8+
- Poetry environment with scanner dependencies
- Supabase credentials in environment variables

This script ensures your **Live Performance Tracking** dashboard shows only meaningful, actionable trade data! üéØ
